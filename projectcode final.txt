import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
# from sklearn.linear_model import Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from statsmodels.stats.outliers_influence import variance_inflation_factor 
import pickle

import warnings
from warnings import filterwarnings
filterwarnings("ignore")

sns.set()

# Load the Calories dataset
df1 = pd.read_csv("D:\\ENGG\\6th sem\\project\\calories.csv")
print(df1.head())
print(df1.shape)

# Load the Exercise Dataset
df2 = pd.read_csv("D:\\ENGG\\6th sem\\project\\exercise.csv")
print(df2.head())
print(df2.shape)

df = pd.concat([df2, df1["Calories"]], axis=1)
print(df.head())
print(df.info())
print(df.describe())
print(df.isnull().sum())

# Drop User_ID column because this is not required
df.drop(columns=["User_ID"], inplace=True)
print(df.head())
print(df.info())

# Fetching Categorical Data
cat_col = [col for col in df.columns if df[col].dtype == 'O']
print(cat_col)
print(df["Gender"].value_counts())

sns.countplot(x=df['Gender'])
plt.show()

categorical = pd.get_dummies(df[cat_col], drop_first=True)
print(categorical.head())

num_col = [col for col in df.columns if df[col].dtype != 'O']
print(num_col)
print(df[num_col].shape)

numerical = df[num_col]
print(numerical.head())
print(numerical.shape)

plt.figure(figsize=(20, 15))
plotnumber = 1

for column in numerical:
    if plotnumber <= 8:
        ax = plt.subplot(3, 3, plotnumber)
        sns.distplot(numerical[column])
        plt.xlabel(column, fontsize=15)
    plotnumber += 1

plt.show()

# Constructing a heatmap to understand the correlation
plt.figure(figsize=(10, 10))
sns.heatmap(numerical.corr(), cmap='Blues', annot=True)
plt.show()

data = pd.concat([categorical, numerical], axis=1)
print(data.head())

fig, ax = plt.subplots(figsize=(15, 10))
sns.boxplot(data=data, width=0.5, fliersize=3, ax=ax)
plt.show()

plt.figure(figsize=(20, 15))
plotnumber = 1

for column in data:
    if plotnumber <= 8:
        ax = plt.subplot(3, 3, plotnumber)
        sns.distplot(data[column])
        plt.xlabel(column, fontsize=15)
    plotnumber += 1

plt.show()
print(data.columns)

X = data.drop(columns=["Calories"])
y = data["Calories"]
print(X.head())
print(y.head())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
print("Shape of X Train: ", X_train.shape)
print("Shape of X Test: ", X_test.shape)
print("Shape of y Train: ", y_train.shape)
print("Shape of y Test: ", y_test.shape)

def predict(ml_model):
    model = ml_model.fit(X_train, y_train)
    print('Score : {}'.format(model.score(X_train, y_train)))
    y_prediction = model.predict(X_test)
    print('Predictions are: \n{}'.format(y_prediction))
    print('\n')

    r2_score = metrics.r2_score(y_test, y_prediction)
    print('R2 Score: {}'.format(r2_score))

    print('MAE:', metrics.mean_absolute_error(y_test, y_prediction))
    print('MSE:', metrics.mean_squared_error(y_test, y_prediction))
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_prediction)))
    
    sns.distplot(y_test - y_prediction)
    plt.show()

regression = XGBRegressor()
predict(regression)

# Saving the model to the local file system
filename = 'finalized_model.pickle'
pickle.dump(regression, open(filename, 'wb'))

predict(LinearRegression())
predict(DecisionTreeRegressor())
predict(RandomForestRegressor())
